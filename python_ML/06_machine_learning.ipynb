{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. tensorflow module 설치(cpu 용)\n",
    "##     > conda install tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "## 2. Hello World 출력\n",
    "##    상수를 하나 만들어요(상수 Node 생성)\n",
    "##    Tensorflow Node는 숫자 연산과 데이터 입출력을 담당\n",
    "##    Session을 이용해서 Node를 실행시켜야지\n",
    "##    Node가 가지고 있는 데이터를 출력할 수 있어요!\n",
    "\n",
    "my_node = tf.constant(\"Hello World\")\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(my_node)\n",
    "print(sess.run(my_node).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TensorFlow : Google이 만든 machine library\n",
    "##              open source library\n",
    "##              수학적 계산을 하기 위한 library\n",
    "##              data flow graph를 이용해요!\n",
    "\n",
    "## data flow graph는 Node와 Edge로 구성된 방향성 있는\n",
    "## graph\n",
    "\n",
    "## Node : 데이터의 입출력과 수학적 연산\n",
    "## Edge : Tensor를 Node로 실어 나르는 역할\n",
    "## Tensor : 동적 크기의 다차원 배열을 지칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 20.0, 30.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.constant(10, dtype=tf.float32)\n",
    "node2 = tf.constant(20, dtype=tf.float32)\n",
    "node3 = node1 + node2\n",
    "\n",
    "# 그래프를 실행시키는 거에요. 그래프를 실행시키기 위해서 특정 러너가 필요해요\n",
    "## 그래프를 실행시키기 위해 runner역할을 하는 Session객체가 있어야 해요\n",
    "\n",
    "sess = tf.Session() # 얘가 있어야지 우리가 위해서 그린 그래프를 \"실행\"시킬 수 있습니다.\n",
    "\n",
    "# print(sess.run(node3)) # node3을 실행하면 node1과 node2가 자동으로 실행되 계산된다음 node3이 실행된다~!\n",
    "print(sess.run([node1, node2, node3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# placeholder를 이용\n",
    "# 2개의 수를 입력으로 받아서 더하는 프로그램\n",
    "node1 = tf.placeholder(dtype=tf.float32) # 데이터를 가지고있는 장소 // 입력 파라메터를 받아 들이기 위한 변수 // 받아와서 꽝 하고 받가놓는거\n",
    "node2 = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "node3 = node1 + node2\n",
    "sess=tf.Session()\n",
    "result = sess.run(node3, feed_dict={node1:10, node2:20}) # node1이랑 node2에 값넣어주기~\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_52:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"Cast_7:0\", shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.constant([10,20,30], dtype=tf.int32)\n",
    "print(node1)\n",
    "node2 = tf.cast(node1, dtype=tf.float32) # node1을 형변환 하겠따 float32로 바꿔서 node2에 저☆장\n",
    "print(node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[-0.77335095], [-0.62896013], 24.71033477783203\n",
      "[0.9729401], [0.06151364], 0.0005454455967992544\n",
      "[0.9868555], [0.02988051], 0.00012870087812189013\n",
      "[0.99361503], [0.01451453], 3.036792804778088e-05\n",
      "[0.9968984], [0.00705055], 7.16581553206197e-06\n",
      "[0.9984933], [0.00342493], 1.6907873714444577e-06\n",
      "[0.99926805], [0.00166391], 3.990714958490571e-07\n",
      "[0.99964416], [0.00080876], 9.43094491390184e-08\n",
      "[0.9998267], [0.00039348], 2.2345318484440213e-08\n",
      "[0.99991566], [0.00019159], 5.293524285576723e-09\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x = [1,2,3]\n",
    "y = [1,2,3] # label\n",
    "\n",
    "# 선형회귀(linear regression)\n",
    "# 가장 큰 목표는 가설의 완성\n",
    "# 가설(hypothesis) = Wx + b\n",
    "# W와 b를 정의\n",
    "# Weight & bias 정의\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\") # tensor node\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\") # tensor node\n",
    "\n",
    "# Hypothesis(가설)\n",
    "# 우리의 최종 목적은 training data에 가장 근접한\n",
    "# Hypothesis를 만드는 것(W와 b를 결정)\n",
    "# 잘 만들어진 가설은 W가 1에 가깝고, b가 0에 가까워야 해요\n",
    "H = W * x + b\n",
    "\n",
    "# cost(loss) function\n",
    "# cost는 0으로 수렴 // train을 한번하면 cost값이 줄어들어 0이면 좋은거네 accuracy가 올라감 \n",
    "################### 우리의 목적은 cost 함수를 최소로 만드는 W와 b를 구하는 거에요 ##########################\n",
    "cost = tf.reduce_mean(tf.square(H - y)) # 가설에서 우리한테 제공된 y 값을 빼서 제곱의 으으으으음~\n",
    "\n",
    "## cost function minimize \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "## runner 생성\n",
    "sess = tf.Session()\n",
    "## global variable의 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "## 학습진행\n",
    "for step in range(3000):\n",
    "    _, w_val, b_val, cost_val = sess.run([train,W,b,cost])\n",
    "    if step % 300 == 0:\n",
    "        print(\"{}, {}, {}\" .format(w_val, b_val, cost_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.92273\n",
      "6.700222e-06\n",
      "1.1100875e-06\n",
      "1.8413726e-07\n",
      "3.067953e-08\n",
      "5.107566e-09\n",
      "8.618599e-10\n",
      "1.9953461e-10\n",
      "6.7757355e-11\n",
      "6.7757355e-11\n",
      "[901.0022]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "x_data = [1,2,3,4]    #               100\n",
    "y_data = [4,7,10,13]  # label data    301\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\") # 상수값이아니고 계속 변해야 하는 값 // 초기값 줘야함\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "H = W * x + b\n",
    "\n",
    "# cost(loss) function\n",
    "cost = tf.reduce_mean(tf.square(H-y)) # 이차함수 모향이구만 이거의 최소값을 찾는게 목표?\n",
    "\n",
    "# cost function을 최소화 시키기 위한 작업\n",
    "# 경사하강법 : 산정상에서 최단시간 하산하기위해 내 위치에서 주변을 둘러봐 경사가 가장 급한곳으로 내려가 다시거기서 주변을 둘러봐 경사가 가장급한곳으로 내려가기를 반복\n",
    "# 그래프에서 미분작업을 통해 가장 경사가 급한 곳을 찾는겨\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # learning_rate가 작으면 현재위치에서 쬐끔만 움직이는 느낌!\n",
    "                                                                  # 너무작으면 실제로 아래까지 내려가는데 한참걸리고 너무 크면 엄한데로 갈수 있다.\n",
    "optimizer.minimize(cost) # cost함수를 최소화 시키는겁니다 // 그러나 한번 슥 들러본다고 경사가 가장낮은곳을 찾을수 있나요 ? 없어요!\n",
    "                         # 그래서 이걸 반복해야됨\n",
    "    \n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session() # 러너 만들고\n",
    "sess.run(tf.global_variables_initializer()) # 전역변수 초기화\n",
    "\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={x:x_data, y:y_data}) # 변하는 x,y_data값 넣어서 cost값만 찍어보겟다\n",
    "\n",
    "    if step % 300 == 0:\n",
    "         print(cost_val)\n",
    "            \n",
    "# prediction\n",
    "print(sess.run(H, feed_dict={x : [300]}))\n",
    "\n",
    "# 우리가 사용하는 cost함수가 만약 3차 이상의 함수면 잘못된 결과 나올수 있따리\n",
    "# cost function은 convex function 형태가 되어야 gradient descent algorithm을 사용할 수 있따"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/ozone/ozone.csv\", sep=\",\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 2)\n",
      "(116, 2)\n"
     ]
    }
   ],
   "source": [
    "## 온도에 따른 오존량 예측\n",
    "## 필요한 컬럼만 일단 추출\n",
    "df2 = df[[\"Ozone\",\"Temp\"]]\n",
    "## 결치값을 처리(제거)\n",
    "df3 = df2.dropna(how=\"any\", inplace=False)\n",
    "print(df2.shape)\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이렇게 준비한 데이터가 linear한 데이터인지 확인\n",
    "plt.scatter(df3[\"Temp\"],df3[\"Ozone\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 1.3159098625183105\n",
      "cost : 0.019994698464870453\n",
      "cost : 0.019817780703306198\n",
      "cost : 0.01981683447957039\n",
      "cost : 0.01981682889163494\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# training data set\n",
    "# 데이터 정제\n",
    "x_data = df3[\"Temp\"]\n",
    "y_data = df3[\"Ozone\"]\n",
    "x_data = (df3[\"Temp\"]-df3[\"Temp\"].min())/(df3[\"Temp\"].max()-df3[\"Temp\"].min())\n",
    "y_data = (df3[\"Ozone\"]-df3[\"Ozone\"].min())/(df3[\"Ozone\"].max()-df3[\"Ozone\"].min())\n",
    "\n",
    "# Weithgt & bias\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "H = W * x + b\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.square(H-y))\n",
    "\n",
    "# 최소화 노드생성\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습(train)\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={x:x_data, y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(\"cost : {}\" .format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ozone</th>\n",
       "      <th>Solar.R</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.3</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.9</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>194.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.9</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>266.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>115.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>96.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>91</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>78.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>92</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>73.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>93</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>91.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>93</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>47.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>32.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>20.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>23.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>21.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>24.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>73</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>44.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>28.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>13.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>46.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>18.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>24.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>16.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>13.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>36.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>30.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>145.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>14.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>18.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>20.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ozone  Solar.R  Wind  Temp  Month  Day\n",
       "0     41.0    190.0   7.4    67      5    1\n",
       "1     36.0    118.0   8.0    72      5    2\n",
       "2     12.0    149.0  12.6    74      5    3\n",
       "3     18.0    313.0  11.5    62      5    4\n",
       "4      NaN      NaN  14.3    56      5    5\n",
       "5     28.0      NaN  14.9    66      5    6\n",
       "6     23.0    299.0   8.6    65      5    7\n",
       "7     19.0     99.0  13.8    59      5    8\n",
       "8      8.0     19.0  20.1    61      5    9\n",
       "9      NaN    194.0   8.6    69      5   10\n",
       "10     7.0      NaN   6.9    74      5   11\n",
       "11    16.0    256.0   9.7    69      5   12\n",
       "12    11.0    290.0   9.2    66      5   13\n",
       "13    14.0    274.0  10.9    68      5   14\n",
       "14    18.0     65.0  13.2    58      5   15\n",
       "15    14.0    334.0  11.5    64      5   16\n",
       "16    34.0    307.0  12.0    66      5   17\n",
       "17     6.0     78.0  18.4    57      5   18\n",
       "18    30.0    322.0  11.5    68      5   19\n",
       "19    11.0     44.0   9.7    62      5   20\n",
       "20     1.0      8.0   9.7    59      5   21\n",
       "21    11.0    320.0  16.6    73      5   22\n",
       "22     4.0     25.0   9.7    61      5   23\n",
       "23    32.0     92.0  12.0    61      5   24\n",
       "24     NaN     66.0  16.6    57      5   25\n",
       "25     NaN    266.0  14.9    58      5   26\n",
       "26     NaN      NaN   8.0    57      5   27\n",
       "27    23.0     13.0  12.0    67      5   28\n",
       "28    45.0    252.0  14.9    81      5   29\n",
       "29   115.0    223.0   5.7    79      5   30\n",
       "..     ...      ...   ...   ...    ...  ...\n",
       "123   96.0    167.0   6.9    91      9    1\n",
       "124   78.0    197.0   5.1    92      9    2\n",
       "125   73.0    183.0   2.8    93      9    3\n",
       "126   91.0    189.0   4.6    93      9    4\n",
       "127   47.0     95.0   7.4    87      9    5\n",
       "128   32.0     92.0  15.5    84      9    6\n",
       "129   20.0    252.0  10.9    80      9    7\n",
       "130   23.0    220.0  10.3    78      9    8\n",
       "131   21.0    230.0  10.9    75      9    9\n",
       "132   24.0    259.0   9.7    73      9   10\n",
       "133   44.0    236.0  14.9    81      9   11\n",
       "134   21.0    259.0  15.5    76      9   12\n",
       "135   28.0    238.0   6.3    77      9   13\n",
       "136    9.0     24.0  10.9    71      9   14\n",
       "137   13.0    112.0  11.5    71      9   15\n",
       "138   46.0    237.0   6.9    78      9   16\n",
       "139   18.0    224.0  13.8    67      9   17\n",
       "140   13.0     27.0  10.3    76      9   18\n",
       "141   24.0    238.0  10.3    68      9   19\n",
       "142   16.0    201.0   8.0    82      9   20\n",
       "143   13.0    238.0  12.6    64      9   21\n",
       "144   23.0     14.0   9.2    71      9   22\n",
       "145   36.0    139.0  10.3    81      9   23\n",
       "146    7.0     49.0  10.3    69      9   24\n",
       "147   14.0     20.0  16.6    63      9   25\n",
       "148   30.0    193.0   6.9    70      9   26\n",
       "149    NaN    145.0  13.2    77      9   27\n",
       "150   14.0    191.0  14.3    75      9   28\n",
       "151   18.0    131.0   8.0    76      9   29\n",
       "152   20.0    223.0  11.5    68      9   30\n",
       "\n",
       "[153 rows x 6 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization : (요소값 - 최소값) / (최대값 - 최소값)\n",
    "# standardization : (요소값 - 평균) / 표준편차\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84699.914\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# training data set \n",
    "x_data = [[73,80,75],  # 열의 갯수는 변하지 않아유~ 사람이 많아지면 행의 갯수가 많아지지\n",
    "         [93,88,93],\n",
    "         [89,91,90],\n",
    "         [96,98,100],\n",
    "         [73,66,70]]\n",
    "\n",
    "y_data = [[152],[185],[180],[196],[142]]\n",
    "\n",
    "# placeholder # 입력받아 쓰는 변수!?\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32) # 행의 갯수는 변할 수 있기 때문에, None(행의 갯수는 상관하지 않겠어!)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([3,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "# H = W * x + b # 일반 숫자 곱\n",
    "H = tf.matmul(X,W) + b # 행렬 곱 함수! // X가 먼저임에 주의\n",
    "\n",
    "# Cost function\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "# 학습노드 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(111, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ozone</th>\n",
       "      <th>Solar.R</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.3</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.9</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>194.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.9</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>266.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>115.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>96.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>91</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>78.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>92</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>73.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>93</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>91.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>93</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>47.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>32.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>20.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>23.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>21.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>24.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>73</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>44.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>28.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>13.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>46.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>18.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>24.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>16.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>13.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>36.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>30.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>145.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>14.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>18.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>20.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ozone  Solar.R  Wind  Temp  Month  Day\n",
       "0     41.0    190.0   7.4    67      5    1\n",
       "1     36.0    118.0   8.0    72      5    2\n",
       "2     12.0    149.0  12.6    74      5    3\n",
       "3     18.0    313.0  11.5    62      5    4\n",
       "4      NaN      NaN  14.3    56      5    5\n",
       "5     28.0      NaN  14.9    66      5    6\n",
       "6     23.0    299.0   8.6    65      5    7\n",
       "7     19.0     99.0  13.8    59      5    8\n",
       "8      8.0     19.0  20.1    61      5    9\n",
       "9      NaN    194.0   8.6    69      5   10\n",
       "10     7.0      NaN   6.9    74      5   11\n",
       "11    16.0    256.0   9.7    69      5   12\n",
       "12    11.0    290.0   9.2    66      5   13\n",
       "13    14.0    274.0  10.9    68      5   14\n",
       "14    18.0     65.0  13.2    58      5   15\n",
       "15    14.0    334.0  11.5    64      5   16\n",
       "16    34.0    307.0  12.0    66      5   17\n",
       "17     6.0     78.0  18.4    57      5   18\n",
       "18    30.0    322.0  11.5    68      5   19\n",
       "19    11.0     44.0   9.7    62      5   20\n",
       "20     1.0      8.0   9.7    59      5   21\n",
       "21    11.0    320.0  16.6    73      5   22\n",
       "22     4.0     25.0   9.7    61      5   23\n",
       "23    32.0     92.0  12.0    61      5   24\n",
       "24     NaN     66.0  16.6    57      5   25\n",
       "25     NaN    266.0  14.9    58      5   26\n",
       "26     NaN      NaN   8.0    57      5   27\n",
       "27    23.0     13.0  12.0    67      5   28\n",
       "28    45.0    252.0  14.9    81      5   29\n",
       "29   115.0    223.0   5.7    79      5   30\n",
       "..     ...      ...   ...   ...    ...  ...\n",
       "123   96.0    167.0   6.9    91      9    1\n",
       "124   78.0    197.0   5.1    92      9    2\n",
       "125   73.0    183.0   2.8    93      9    3\n",
       "126   91.0    189.0   4.6    93      9    4\n",
       "127   47.0     95.0   7.4    87      9    5\n",
       "128   32.0     92.0  15.5    84      9    6\n",
       "129   20.0    252.0  10.9    80      9    7\n",
       "130   23.0    220.0  10.3    78      9    8\n",
       "131   21.0    230.0  10.9    75      9    9\n",
       "132   24.0    259.0   9.7    73      9   10\n",
       "133   44.0    236.0  14.9    81      9   11\n",
       "134   21.0    259.0  15.5    76      9   12\n",
       "135   28.0    238.0   6.3    77      9   13\n",
       "136    9.0     24.0  10.9    71      9   14\n",
       "137   13.0    112.0  11.5    71      9   15\n",
       "138   46.0    237.0   6.9    78      9   16\n",
       "139   18.0    224.0  13.8    67      9   17\n",
       "140   13.0     27.0  10.3    76      9   18\n",
       "141   24.0    238.0  10.3    68      9   19\n",
       "142   16.0    201.0   8.0    82      9   20\n",
       "143   13.0    238.0  12.6    64      9   21\n",
       "144   23.0     14.0   9.2    71      9   22\n",
       "145   36.0    139.0  10.3    81      9   23\n",
       "146    7.0     49.0  10.3    69      9   24\n",
       "147   14.0     20.0  16.6    63      9   25\n",
       "148   30.0    193.0   6.9    70      9   26\n",
       "149    NaN    145.0  13.2    77      9   27\n",
       "150   14.0    191.0  14.3    75      9   28\n",
       "151   18.0    131.0   8.0    76      9   29\n",
       "152   20.0    223.0  11.5    68      9   30\n",
       "\n",
       "[153 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5667481\n",
      "0.0738212\n",
      "0.042762887\n",
      "0.029237391\n",
      "0.023088977\n",
      "0.02012611\n",
      "0.018587505\n",
      "0.017715653\n",
      "0.017174896\n",
      "0.016810812\n"
     ]
    }
   ],
   "source": [
    "df2=df.dropna(how=\"any\", inplace=False)\n",
    "display(df.shape)\n",
    "display(df2.shape)\n",
    "display(df)\n",
    "\n",
    "x_data = df2[[\"Solar.R\",\"Wind\",\"Temp\"]]\n",
    "y_data = df2[[\"Ozone\"]].values.reshape(-1,1)\n",
    "\n",
    "normalized_x_data = (x_data-x_data.min())/(x_data.max()-x_data.min())\n",
    "normalized_y_data = (y_data-y_data.min())/(y_data.max()-y_data.min())\n",
    "\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "H = tf.matmul(X,W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:normalized_x_data, Y:normalized_y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39876223\n",
      "0.08261634\n",
      "0.04218209\n",
      "0.026251981\n",
      "0.01994464\n",
      "0.017424652\n",
      "0.016400881\n",
      "0.015972028\n",
      "0.015782459\n",
      "0.015691163\n",
      "0.015641741\n",
      "0.015611324\n",
      "0.015590376\n",
      "0.0155747635\n",
      "0.015562544\n",
      "0.01555272\n",
      "0.015544706\n",
      "0.015538117\n",
      "0.015532681\n",
      "0.015528185\n",
      "0.015524459\n",
      "0.015521376\n",
      "0.015518814\n",
      "0.015516694\n",
      "0.015514937\n",
      "0.015513479\n",
      "0.015512268\n",
      "0.015511263\n",
      "0.015510432\n",
      "0.015509741\n",
      "0.01550917\n",
      "0.015508695\n",
      "0.0155083025\n",
      "0.015507973\n",
      "0.015507703\n",
      "0.015507476\n",
      "0.015507291\n",
      "0.0155071365\n",
      "0.015507009\n",
      "0.015506901\n",
      "0.015506814\n",
      "0.015506739\n",
      "0.01550668\n",
      "0.015506628\n",
      "0.015506589\n",
      "0.0155065525\n",
      "0.015506525\n",
      "0.0155065\n",
      "0.015506481\n",
      "0.015506465\n",
      "0.015506451\n",
      "0.015506438\n",
      "0.015506429\n",
      "0.015506423\n",
      "0.015506415\n",
      "0.015506412\n",
      "0.015506408\n",
      "0.0155064035\n",
      "0.0155064\n",
      "0.015506396\n",
      "0.015506395\n",
      "0.015506393\n",
      "0.015506391\n",
      "0.015506391\n",
      "0.015506389\n",
      "0.015506389\n",
      "0.015506391\n",
      "0.015506388\n",
      "0.015506387\n",
      "0.015506387\n",
      "0.015506388\n",
      "0.015506386\n",
      "0.015506386\n",
      "0.015506385\n",
      "0.015506385\n",
      "0.015506385\n",
      "0.015506382\n",
      "0.015506387\n",
      "0.015506385\n",
      "0.015506385\n",
      "0.015506384\n",
      "0.015506385\n",
      "0.015506386\n",
      "0.015506382\n",
      "0.015506386\n",
      "0.015506384\n",
      "0.015506386\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "0.015506384\n",
      "[[46.181705]]\n",
      "==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Solar.R</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>313.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>299.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>99.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>256.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>290.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>274.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>65.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>334.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>307.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>78.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>322.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>320.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>92.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>252.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>223.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>279.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>127.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>291.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>323.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>148.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>191.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>284.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>188.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>167.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>197.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>183.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>189.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>95.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>92.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>252.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>220.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>230.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>259.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>236.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>259.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>238.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>24.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>112.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>237.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>224.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>27.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>238.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>201.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>238.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>14.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>139.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>49.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>20.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>193.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>191.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>131.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>223.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Solar.R  Wind  Temp\n",
       "0      190.0   7.4    67\n",
       "1      118.0   8.0    72\n",
       "2      149.0  12.6    74\n",
       "3      313.0  11.5    62\n",
       "6      299.0   8.6    65\n",
       "7       99.0  13.8    59\n",
       "8       19.0  20.1    61\n",
       "11     256.0   9.7    69\n",
       "12     290.0   9.2    66\n",
       "13     274.0  10.9    68\n",
       "14      65.0  13.2    58\n",
       "15     334.0  11.5    64\n",
       "16     307.0  12.0    66\n",
       "17      78.0  18.4    57\n",
       "18     322.0  11.5    68\n",
       "19      44.0   9.7    62\n",
       "20       8.0   9.7    59\n",
       "21     320.0  16.6    73\n",
       "22      25.0   9.7    61\n",
       "23      92.0  12.0    61\n",
       "27      13.0  12.0    67\n",
       "28     252.0  14.9    81\n",
       "29     223.0   5.7    79\n",
       "30     279.0   7.4    76\n",
       "37     127.0   9.7    82\n",
       "39     291.0  13.8    90\n",
       "40     323.0  11.5    87\n",
       "43     148.0   8.0    82\n",
       "46     191.0  14.9    77\n",
       "47     284.0  20.7    72\n",
       "..       ...   ...   ...\n",
       "122    188.0   6.3    94\n",
       "123    167.0   6.9    91\n",
       "124    197.0   5.1    92\n",
       "125    183.0   2.8    93\n",
       "126    189.0   4.6    93\n",
       "127     95.0   7.4    87\n",
       "128     92.0  15.5    84\n",
       "129    252.0  10.9    80\n",
       "130    220.0  10.3    78\n",
       "131    230.0  10.9    75\n",
       "132    259.0   9.7    73\n",
       "133    236.0  14.9    81\n",
       "134    259.0  15.5    76\n",
       "135    238.0   6.3    77\n",
       "136     24.0  10.9    71\n",
       "137    112.0  11.5    71\n",
       "138    237.0   6.9    78\n",
       "139    224.0  13.8    67\n",
       "140     27.0  10.3    76\n",
       "141    238.0  10.3    68\n",
       "142    201.0   8.0    82\n",
       "143    238.0  12.6    64\n",
       "144     14.0   9.2    71\n",
       "145    139.0  10.3    81\n",
       "146     49.0  10.3    69\n",
       "147     20.0  16.6    63\n",
       "148    193.0   6.9    70\n",
       "150    191.0  14.3    75\n",
       "151    131.0   8.0    76\n",
       "152    223.0  11.5    68\n",
       "\n",
       "[111 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## multiple linear regression\n",
    "## Ozone Data 학슴 및 예측\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv(\"./data/ozone/ozone.csv\", sep =\",\")\n",
    "\n",
    "# 필요한 컬럼만 추출\n",
    "df.drop([\"Month\", \"Day\"],axis=1, inplace=True)\n",
    "# 결치값 처리(제거)\n",
    "df.dropna(how=\"any\", inplace=True)\n",
    "# x 데이터 추출\n",
    "df_x = df.drop(\"Ozone\",axis=1,inplace=False)\n",
    "df_y = df[\"Ozone\"]\n",
    "df_y  ##### 주의 ##### 컬럼하나를 당겼기 때문에 series로 나와요!!! x축은 dataframe형태 y축은 series형태로 나와요\n",
    "\n",
    "# 뽑히는 형태는 dataframe 우리가 필요한건 안의 실제 데이터 값 이거뽑으려면 어케해야되요 ?\n",
    "# 기억나면 이상한거에요 그러나 기억해야되요  # df_x.values\n",
    "# training data set\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_value = df_y.values # 1차원 배열형태로 빠지게 되요 백터형태로\n",
    "                     # 그러나 우리가필요한건 2차원 메트릭스 형태!!\n",
    "y_data = MinMaxScaler().fit_transform(y_value.reshape(-1,1)) # 요로코롬하면 2차원 메트릭스로 바뀜\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([3,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "H = tf.matmul(X,W) + b\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.square(H-Y))\n",
    "\n",
    "# train node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session&초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 진행\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "# prediction\n",
    "print(sess.run(H,feed_dict = {X:[[190,7.4,67]]})) # 데이터 3개 입력햇을때 오존(H) 이 얼마냐 ?물어보는거\n",
    "\n",
    "print(\"==\")\n",
    "display(df_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.77151\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "[0.50340176]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# training data set\n",
    "x_data = [1,2,5,8,10] # 30시간공부해서 합격한 데이터가 들어오면 문제가 발생함\n",
    "y_data = [0,0,0,1,1]\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "H = W * x + b\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.square(H-y))\n",
    "\n",
    "# train node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={x:x_data, y:y_data})\n",
    "\n",
    "    if step % 3000 == 0:\n",
    "        print(cost_val)\n",
    "\n",
    "# prediction\n",
    "print(sess.run(H, feed_dict={x:[6]}))\n",
    "\n",
    "# plot\n",
    "plt.scatter(x_data,y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "plt.scatter(x_data,y_data)\n",
    "plt.plot(x_data,x_data * sess.run(W) + sess.run(b), \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9450958\n",
      "0.2674254\n",
      "0.2110347\n",
      "0.18509904\n",
      "0.16839913\n",
      "0.15591073\n",
      "0.14580736\n",
      "0.13725129\n",
      "0.1297961\n",
      "0.12317751\n",
      "정확도 : 1.0\n",
      "[[0.8149972]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x_data=[[30,0],\n",
    "        [10,0],\n",
    "        [8,1],\n",
    "        [3,3],\n",
    "        [2,3],\n",
    "        [5,1],\n",
    "        [2,0],\n",
    "        [1,0]]\n",
    "y_data=[[1],[1],[1],[1],[1],[0],[0],[0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([2,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "# H = tf.matmul(X,W) + b\n",
    "logits = tf.matmul(X,W) + b # 아주심플하게 생각하면 직선을 곡선형태로 바꿔주는거!@!@!@@!@!@!@!@!\n",
    "H = tf.sigmoid(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y))\n",
    "\n",
    "# training node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    if step % 3000 == 0:\n",
    "        print(cost_val)\n",
    "\n",
    "      \n",
    "    \n",
    "# Accuracy\n",
    "# 가지고 있는 학습데이터셋을 7:3 학습과 평가를 진행\n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "correct = tf.equal(predict,Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "print(\"정확도 : {}\" .format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})) )\n",
    "\n",
    "# prediction\n",
    "print(sess.run(H, feed_dict={X:[[4,2]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.3174999952316284\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"./data/admission/admission.csv\", sep=\",\")\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# x 데이터 추출\n",
    "df_x = df.drop(\"admit\",axis=1,inplace=False)\n",
    "df_y = df[\"admit\"]\n",
    "\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_value = df_y.values # 1차원 배열형태로 빠지게 되요 백터형태로\n",
    "y_data = MinMaxScaler().fit_transform(y_value.reshape(-1,1)) # 요로코롬하면 2차원 메트릭스로 바뀜\n",
    "\n",
    "\n",
    "# Weithgt & bias\n",
    "W = tf.Variable(tf.random_normal([3,1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "# H = tf.matmul(X,W) + b\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y))\n",
    "\n",
    "# training node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습\n",
    "# for step in range(30000):\n",
    "#     _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "#     if step % 3000 == 0:\n",
    "#         print(cost_val)\n",
    "    \n",
    "# Accuracy\n",
    "    # 가지고 있는 학습데이터셋을 7:3 학습과 평가를 진행\n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "correct = tf.equal(predict,Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "print(\"정확도 : {}\" .format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})) )\n",
    "\n",
    "# prediction\n",
    "# print(sess.run(H, feed_dict={X:[[4,2]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2209947\n",
      "0.4767848\n",
      "0.46508515\n",
      "0.4598119\n",
      "0.4566083\n",
      "0.45442295\n",
      "0.4528536\n",
      "0.4516921\n",
      "0.4508131\n",
      "0.45013562\n",
      "정확도 : 0.7759562730789185\n",
      "[[0.45172134]]\n",
      "     Pclass  Sex  Age  SibSp  Parch\n",
      "1         1    2  3.0      1      0\n",
      "3         1    2  3.0      1      0\n",
      "6         1    1  5.0      0      0\n",
      "10        3    2  0.0      1      1\n",
      "11        1    2  5.0      0      0\n",
      "21        2    1  3.0      0      0\n",
      "23        1    1  2.0      0      0\n",
      "27        1    1  1.0      3      2\n",
      "52        1    2  4.0      1      0\n",
      "54        1    1  6.0      0      1\n",
      "62        1    1  4.0      1      0\n",
      "66        2    2  2.0      0      0\n",
      "75        3    1  2.0      0      0\n",
      "88        1    2  2.0      3      2\n",
      "92        1    1  4.0      1      0\n",
      "96        1    1  7.0      0      0\n",
      "97        1    1  2.0      0      1\n",
      "102       1    1  2.0      0      1\n",
      "110       1    1  4.0      0      0\n",
      "118       1    1  2.0      0      1\n",
      "123       2    2  3.0      0      0\n",
      "124       1    1  5.0      0      1\n",
      "136       1    2  1.0      0      2\n",
      "137       1    1  3.0      1      0\n",
      "139       1    1  2.0      0      0\n",
      "148       2    1  3.0      0      2\n",
      "151       1    2  2.0      1      0\n",
      "170       1    1  6.0      0      0\n",
      "174       1    1  5.0      0      0\n",
      "177       1    2  5.0      0      0\n",
      "..      ...  ...  ...    ...    ...\n",
      "737       1    1  3.0      0      0\n",
      "741       1    1  3.0      1      0\n",
      "742       1    2  2.0      2      2\n",
      "745       1    1  7.0      1      1\n",
      "748       1    1  1.0      1      0\n",
      "751       3    1  0.0      0      1\n",
      "759       1    2  3.0      0      0\n",
      "763       1    2  3.0      1      2\n",
      "765       1    2  5.0      1      0\n",
      "772       2    2  5.0      0      0\n",
      "779       1    2  4.0      0      1\n",
      "781       1    2  1.0      1      0\n",
      "782       1    1  2.0      0      0\n",
      "789       1    1  4.0      0      0\n",
      "796       1    2  4.0      0      0\n",
      "802       1    1  1.0      1      2\n",
      "806       1    1  3.0      0      0\n",
      "809       1    2  3.0      1      0\n",
      "820       1    2  5.0      1      1\n",
      "823       3    2  2.0      0      1\n",
      "835       1    2  3.0      1      1\n",
      "853       1    2  1.0      0      1\n",
      "857       1    1  5.0      0      0\n",
      "862       1    2  4.0      0      0\n",
      "867       1    1  3.0      0      0\n",
      "871       1    2  4.0      1      1\n",
      "872       1    1  3.0      0      0\n",
      "879       1    2  5.0      0      1\n",
      "887       1    2  1.0      0      0\n",
      "889       1    1  2.0      0      0\n",
      "\n",
      "[183 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"./data/titanic/train.csv\", sep=\",\")\n",
    "\n",
    "\n",
    "df.dropna(how=\"any\", inplace=True) # NaN이 있는 모든 row 삭제\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,5], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# x 데이터 추출\n",
    "df_x = df[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\"]]\n",
    "df_y = df[\"Survived\"]\n",
    "\n",
    "df_x.dropna(how=\"any\", inplace=True)\n",
    "df_x.loc[df_x[\"Sex\"]==\"male\",\"Sex\"] = 1 # 남자 1\n",
    "df_x.loc[df_x[\"Sex\"]==\"female\",\"Sex\"] = 2 # 여자 2\n",
    "\n",
    "\n",
    "df_x.loc[:,\"Age\"] = df_x[\"Age\"]//10\n",
    "\n",
    "# 기준선(hyperplane)을 찾아내는게 로지스틱 머쩌구\n",
    "\n",
    "\n",
    "\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_value = df_y.values # 1차원 배열형태로 빠지게 되요 백터형태로\n",
    "y_data = MinMaxScaler().fit_transform(y_value.reshape(-1,1)) # 요로코롬하면 2차원 메트릭스로 바뀜\n",
    "\n",
    "\n",
    "# Weithgt & bias\n",
    "W = tf.Variable(tf.random_normal([5,1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "# H = tf.matmul(X,W) + b\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y))\n",
    "\n",
    "# training node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 3000 == 0:\n",
    "        print(cost_val)\n",
    "    \n",
    "# Accuracy\n",
    "# 가지고 있는 학습데이터셋을 7:3 학습과 평가를 진행\n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "correct = tf.equal(predict,Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "print(\"정확도 : {}\" .format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})) )\n",
    "\n",
    "# prediction\n",
    "print(sess.run(H, feed_dict={X:[[1,2,2.6,0,0]]}))\n",
    "print(df_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.588735\n",
      "0.40939024\n",
      "0.2952256\n",
      "0.22702466\n",
      "0.18242733\n",
      "0.15141909\n",
      "0.1288286\n",
      "0.111752726\n",
      "0.09845615\n",
      "0.087846\n",
      "정확도 :1.0\n",
      "[[8.5941297e-01 1.4058703e-01 3.7286267e-09]]\n"
     ]
    }
   ],
   "source": [
    "# multinomial classfication\n",
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x_data = [[10,7,8,5],\n",
    "         [8,8,9,4],\n",
    "         [7,8,2,3],\n",
    "         [6,3,9,3],\n",
    "         [7,5,7,4],\n",
    "         [3,5,6,2],\n",
    "         [2,4,3,1]]\n",
    "\n",
    "y_data = [[1,0,0], # one-hot encoding\n",
    "         [1,0,0],\n",
    "         [0,1,0],\n",
    "         [0,1,0],\n",
    "         [0,1,0],\n",
    "         [0,0,1],\n",
    "         [0,0,1]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,4],dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,3],dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([4,3]),name=\"weight\") # 100 010 001 세개 있어서 그런듯\n",
    "b = tf.Variable(tf.random_normal([3]),name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logits)\n",
    "\n",
    "# Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# 이하 학습까지 동일코드\n",
    "# training node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    if step % 3000 == 0:\n",
    "        print(cost_val)\n",
    "    \n",
    "# Accuracy\n",
    "# logistic => H가 0~1사이의 실수로 값 산출\n",
    "#multinomial => (확률, 확률, 확률)\n",
    "# 예) (0.4, 0.5, 0.1) =>\n",
    "predict = tf.argmax(H,1) # 가장 큰 값이 어느 인덱스에 있는가를 알려줍니다\n",
    "                         # 2차원에서 축은 가로방향\n",
    "correct = tf.equal(predict, tf.argmax(Y,1)) # True가 나왔따면, 내가 추정한 데이터와 같다는 얘기 => 예측이 된다는 얘기 ?\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"정확도 :{}\".format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})))\n",
    "\n",
    "# prediction\n",
    "print(sess.run(H, feed_dict={X:[[10,7,8,5]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        2\n",
       "2        0\n",
       "3        2\n",
       "4        1\n",
       "5        2\n",
       "6        2\n",
       "7        0\n",
       "8        2\n",
       "9        2\n",
       "10       2\n",
       "11       0\n",
       "12       1\n",
       "13       2\n",
       "14       2\n",
       "15       1\n",
       "16       0\n",
       "17       2\n",
       "18       2\n",
       "19       1\n",
       "20       2\n",
       "21       0\n",
       "22       0\n",
       "23       2\n",
       "24       0\n",
       "25       1\n",
       "26       1\n",
       "27       0\n",
       "28       0\n",
       "29       1\n",
       "        ..\n",
       "13971    2\n",
       "13972    2\n",
       "13973    0\n",
       "13974    2\n",
       "13975    2\n",
       "13976    2\n",
       "13977    1\n",
       "13978    1\n",
       "13979    2\n",
       "13980    1\n",
       "13981    0\n",
       "13982    0\n",
       "13983    2\n",
       "13984    2\n",
       "13985    2\n",
       "13986    1\n",
       "13987    2\n",
       "13988    2\n",
       "13989    2\n",
       "13990    1\n",
       "13991    2\n",
       "13992    0\n",
       "13993    1\n",
       "13994    1\n",
       "13995    2\n",
       "13996    2\n",
       "13997    0\n",
       "13998    1\n",
       "13999    2\n",
       "14000    0\n",
       "Name: label, Length: 14001, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3804531\n",
      "0.05331658\n",
      "0.05223701\n",
      "0.051285114\n",
      "0.050435852\n",
      "0.049672164\n",
      "0.04898077\n",
      "0.04835119\n",
      "0.04777488\n",
      "0.047244985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.7745203e-04, 9.9962258e-01, 4.5483830e-09],\n",
       "       [4.1172229e-14, 3.0384013e-02, 9.6961606e-01],\n",
       "       [9.9995053e-01, 4.9504204e-05, 7.2969104e-20],\n",
       "       ...,\n",
       "       [7.9914553e-06, 9.9999154e-01, 4.5407597e-07],\n",
       "       [9.4334377e-17, 5.8081810e-04, 9.9941921e-01],\n",
       "       [9.8835850e-01, 1.1641495e-02, 2.5957388e-13]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.9827155470848083\n",
      "[[1. 0. 0.]]\n",
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"./data/bmi/bmi.csv\", sep=\",\",skiprows=3)\n",
    "\n",
    "df.dropna(how=\"any\", inplace=True) # NaN이 있는 모든 row 삭제\n",
    "split_count = int(df.shape[0] * 0.7)\n",
    "df = df.loc[:split_count,:]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([2,3]),name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([3]),name=\"bias\")\n",
    "\n",
    "# x 데이터 추출\n",
    "df_x = df[[\"height\",\"weight\"]]\n",
    "df_y = df[\"label\"]\n",
    "\n",
    "df_x.dropna(how=\"any\", inplace=True)\n",
    "\n",
    "# 기준선(hyperplane)을 찾아내는게 로지스틱 머쩌구\n",
    "\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_value = df_y.values # 1차원 배열형태로 빠지게 되요 백터형태로\n",
    "y_data = MinMaxScaler().fit_transform(y_value.reshape(-1,1)) # 요로코롬하면 2차원 메트릭스로 바뀜\n",
    "\n",
    "\n",
    "# ONE-HOT Encoding #이거정확히물어보자\n",
    "display(df_y)\n",
    "y_data=tf.one_hot(df_y,3).eval(session=tf.Session())\n",
    "\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "# Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "# 실제값이랑 가설값(H)이 둘이 얼마나 유사한지 계산하는거 0~1값이며 0이면 똑같\n",
    "\n",
    "# 이하 학습까지 동일코드\n",
    "# training node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=30).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(cost_val)\n",
    "    \n",
    "# Accuracy\n",
    "# 가지고 있는 학습데이터셋을 7:3 학습과 평가를 진행\n",
    "predict = tf.argmax(H,1)\n",
    "display(sess.run(H, feed_dict={X:x_data, Y:y_data}))\n",
    "correct = tf.equal(predict,tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\" .format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})) )\n",
    "\n",
    "# prediction\n",
    "print(sess.run(H, feed_dict={X:[[142,41]]}))\n",
    "print(sess.run(H, feed_dict={X:[[141,41]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.951217770576477\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "저체중\n",
      "과체중\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "표준\n",
      "과체중\n",
      "과체중\n",
      "저체중\n",
      "표준\n",
      "과체중\n",
      "저체중\n",
      "저체중\n",
      "과체중\n",
      "표준\n",
      "표준\n",
      "표준\n",
      "[[1. 0. 0.]]\n",
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# multinomial classification -> BMI 예제\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"./data/bmi/bmi.csv\", sep=\",\",skiprows=3)\n",
    "\n",
    "df.dropna(how=\"any\", inplace=True)\n",
    "split_count = int(df.shape[0] * 0.7)\n",
    "train_df = df.loc[:split_count,:]\n",
    "test_df = df.loc[split_count:,:]\n",
    "test_df.drop(\"label\",axis=1,inplace= True)\n",
    "t_data = MinMaxScaler().fit_transform(test_df.values)\n",
    "\n",
    "df_x = train_df.drop(\"label\",axis = 1, inplace = False)\n",
    "# ONE-HOT Encoding\n",
    "df_y = train_df[\"label\"]\n",
    "\n",
    "\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_data = tf.one_hot(df_y,3).eval(session=tf.Session())\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2],dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,3],dtype=tf.float32)\n",
    "\n",
    "# weight & bias\n",
    "# logistic 3개가 모여있다~!\n",
    "# W와 b 모두 3개씩!\n",
    "W = tf.Variable(tf.random_normal([2,3]),name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([3]),name = \"bias\")\n",
    "     \n",
    "# hypothesis\n",
    "logits= tf.matmul(X,W)+b\n",
    "H = tf.nn.softmax(logits)\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y))\n",
    "\n",
    "# training node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost],feed_dict = {X:x_data,Y:y_data})\n",
    "        \n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict,tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype = tf.float32))\n",
    "print(\"Accuracy : {}\".format(sess.run(accuracy,feed_dict = {X :x_data,Y:y_data})))\n",
    "\n",
    "res = sess.run(tf.argmax(sess.run(H, feed_dict = {X:t_data}),1))\n",
    "for i in res:\n",
    "    if i == 0 : \n",
    "        print(\"저체중\")\n",
    "    elif i == 1 :\n",
    "        print(\"표준\")\n",
    "    else : \n",
    "        print(\"과체중\")\n",
    "        \n",
    "# prediction\n",
    "print(sess.run(H, feed_dict={X:[[142,41]]}))\n",
    "print(sess.run(H, feed_dict={X:[[141,41]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.unsupervised learning(비지도 학습)\n",
    "#     => training data에 label이 존재하지 않아요!\n",
    "#     => clustering작업이 일반적으로 진행\n",
    "# 3. 강화학습\n",
    "#     => 상점과 벌점을 이용하여 점점저 더 좋은 방향으로 학습해 나가는 방식\n",
    "\n",
    "## Supervised Learning (지도학습)\n",
    "## 1. single linear regression(단순 선형회귀)\n",
    "## 2. multiple linear regression(다중 선형회귀)\n",
    "##     => matrix\n",
    "## 3. Logistic regression (binary classfication)\n",
    "## 4. Multinomial classfication\n",
    "\n",
    "## ++ 추가 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-f02304cc737a>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "6.3079076\n",
      "2.3312235\n",
      "1.6054523\n",
      "1.3931775\n",
      "1.0675359\n",
      "0.81947875\n",
      "1.1546594\n",
      "1.0987316\n",
      "0.9274542\n",
      "0.97197145\n",
      "정확도 : 0.8366000056266785\n"
     ]
    }
   ],
   "source": [
    "## 기본 MNIST 예제 (multinomial classfication)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "\n",
    "# # Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# # 데이터 확인\n",
    "# print(mnist.train.num_examples) # 학습용 데이터의 개수\n",
    "# print(mnist.train.images.shape) # (55000,784) // 28x28이미지를 1차원 형태로 저장\n",
    "\n",
    "# print(mnist.train.labels.shape)\n",
    "\n",
    "# plt.imshow(mnist.train.images[3].reshape(28,28),cmap=\"Greys\", interpolation=\"nearest\")\n",
    "# plt.show()\n",
    "# # print(mnist.train.labels[0])\n",
    "\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([784,10]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([10]), name=\"bias\")\n",
    "\n",
    "                                     # multinomial softx를 이용해서 각각의 확률을 구하셔야 합니다\n",
    "# Hypotesis\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 사용하는 데이터의 크기가 상당히 커요!\n",
    "# 데이터의 크기에 상관없이 학습하는 방식이 필요!\n",
    "# epoch : traing data를 1번 학습시키는 것.\n",
    "# 학습 진행\n",
    "training_epoch = 30\n",
    "batch_size = 100 # 55000개의 행을 다 읽어들이는게 아니라 100개의 행을 읽어서 반복 학습\n",
    "\n",
    "for step in range(training_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "        _,cost_val = sess.run([train,cost],\n",
    "                              feed_dict = {X:batch_x,\n",
    "                                           Y:batch_y})\n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "        \n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict = {X:mnist.test.images, Y:mnist.test.labels})\n",
    "\n",
    "print(\"정확도 : {}\" .format(result))\n",
    "\n",
    "# Prediction\n",
    "## 종이에 숫자를 하나 써서 스캐너로 읽어들인 후 28*28형태의 펙셀 데이터로 변환.\n",
    "\n",
    "# # 학습 진행\n",
    "# for step in range(3000):\n",
    "#     _, cost_val = sess.run([train,cost], feed_dict={X:mnist.train.images, Y:mnist.train.labels})\n",
    "    \n",
    "#     if step%300 == 0:\n",
    "#         print(cost_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6952172\n",
      "0.54800284\n",
      "0.48927307\n",
      "0.44736207\n",
      "0.41320696\n",
      "0.3845069\n",
      "0.36001247\n",
      "0.3388387\n",
      "0.320326\n",
      "0.30397463\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# logistic regression을 이용하여 AND 연산을 학습\n",
    "#\n",
    "import tensorflow as tf\n",
    "# training data set\n",
    "x_data =[[0,0],\n",
    "        [0,1],\n",
    "        [1,0],\n",
    "        [1,1]]\n",
    "y_data = [[0],[0],[0],[1]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([2,1], name=\"weight\"))\n",
    "b = tf.Variable(tf.random_normal([1], name=\"bias\"))\n",
    "\n",
    "# hypothesis\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels = Y))\n",
    "\n",
    "# train node\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "        \n",
    "# Accuracy 측정\n",
    "predict = tf.cast(H > 0.5, dtype = tf.float32) \n",
    "correct = tf.equal(predict, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "print(\"{}\".format(sess.run(accuracy,\n",
    "                          feed_dict={X:x_data,\n",
    "                                    Y:y_data})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90798664\n",
      "0.67812085\n",
      "0.62482977\n",
      "0.52131605\n",
      "0.37968928\n",
      "0.24969557\n",
      "0.16338745\n",
      "0.112757444\n",
      "0.08264651\n",
      "0.0636868\n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# NN을 이용하여 AND 연산을 학습\n",
    "#\n",
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W1 = tf.Variable(tf.random_normal([2,8]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([8]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([8,1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "\n",
    "# hypothesis\n",
    "logits = tf.matmul(layer1,W2) + b2\n",
    "H = tf.sigmoid(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels = Y))\n",
    "               \n",
    "# train node\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "               \n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost],\n",
    "                          feed_dict={X:x_data,\n",
    "                                     Y:y_data})\n",
    "    if step % 3000 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "# Accuracy 측정\n",
    "predict = tf.cast(H > 0.5, dtype = tf.float32) \n",
    "correct = tf.equal(predict, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy,\n",
    "                          feed_dict={X:x_data,\n",
    "                                    Y:y_data})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000000001C127DA0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000000281BB080>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000000281BB160>)\n",
      "====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-263a9b47c092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m         _,cost_val = sess.run([train,cost],\n\u001b[0;32m     55\u001b[0m                               feed_dict = {X:batch_x,\n\u001b[1;32m---> 56\u001b[1;33m                                            Y:batch_y})\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## MNIST (Neural Network)\n",
    "## tensorflow에 example로 포함된 MNIST예제를\n",
    "## NN으로 학습 ( accuracy => 95% )\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# data loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\",one_hot = True)\n",
    "\n",
    "                                  \n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)  # 784 입렵\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)   # 10 출력\n",
    "\n",
    "# Weight & bias\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]), name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "\n",
    "# hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = Y))\n",
    "               \n",
    "# train node\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "               \n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "         \n",
    "training_epoch = 30\n",
    "batch_size = 100 # 55000개의 행을 다 읽어들이는게 아니라\n",
    "                 # 100개의 행을 읽어서 반복 학습\n",
    "    \n",
    "for step in range(training_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "        _,cost_val = sess.run([train,cost],\n",
    "                              feed_dict = {X:batch_x,\n",
    "                                           Y:batch_y})\n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "# Accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy,\n",
    "                 feed_dict = {X:mnist.test.images,\n",
    "                              Y:mnist.test.labels})\n",
    "print(\"정확도 : {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "10000\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Datasets' object has no attribute 'sample_submission'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-7be3bbbc19e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 학습용 데이터의 개수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (55000,784)  28 * 28 이미지를 1차원 형태로 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Datasets' object has no attribute 'sample_submission'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "\n",
    "df1= pd.read_csv(\"./data/wati/train.csv\", sep=\",\")\n",
    "df2= pd.read_csv(\"./data/wati/test.csv\", sep=\",\")\n",
    "df3= pd.read_csv(\"./data/wati/sample_submission.csv\", sep=\",\")\n",
    "\n",
    "# #데이터정제\n",
    "df.dropna(how=\"any\", inplace=True)\n",
    "# # NaN이 있는 모든 row 삭제\n",
    "\n",
    "split_count = int(df.shape[0] * 0.7)\n",
    "train_df = df.loc[:split_count,:]\n",
    "test_df = df.loc[split_count+1:,:]\n",
    "\n",
    "\n",
    "print(mnist.train.num_examples) # 학습용 데이터의 개수\n",
    "print(mnist.train.images.shape) # (55000,784)  28 * 28 이미지를 1차원 형태로 저장\n",
    "print(mnist.train.labels.shape)\n",
    "\n",
    "print(mnist.test.num_examples) # 학습용 데이터의 개수\n",
    "print(mnist.test.images.shape) # (55000,784)  28 * 28 이미지를 1차원 형태로 저장\n",
    "print(mnist.test.labels.shape)\n",
    "\n",
    "print(mnist.sample_submission.num_examples) # 학습용 데이터의 개수\n",
    "print(mnist.sample_submission.images.shape) # (55000,784)  28 * 28 이미지를 1차원 형태로 저장\n",
    "print(mnist.sample_submission.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv(\"./data/kaggle/train.csv\", sep=\",\")\n",
    "test = pd.read_csv(\"./data/kaggle/test.csv\", sep=\",\")\n",
    "#display(df.head())\n",
    "\n",
    "#데이터 정제\n",
    "Y_train =train[\"label\"]\n",
    "#Drop 'label' column\n",
    "X_train =train.drop(labels=[\"label\"],axis=1)\n",
    "Y_train.value_counts()\n",
    "\n",
    "#Normalize the data\n",
    "X_train = X_train/255.0\n",
    "test = test /255.0\n",
    "\n",
    "#Reshape\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test=test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n"
     ]
    }
   ],
   "source": [
    "## 스나\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 싸이키런\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Data Loading\n",
    "mnist = pd.read_csv('./data/kaggle/train.csv')\n",
    "mnist_test = pd.read_csv('./data/kaggle/test.csv')\n",
    "mnist = input_data.read_data_sets(\"./data/kaggle/train.csv\",one_hot = True)\n",
    "\n",
    "df_x = mnist.drop('label', axis=1, inplace=False)\n",
    "df_y = mnist['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_data = MinMaxScaler().fit_transform(tf.one_hot(df_y, depth=10).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder \n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([256]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "# 두번째  Layer .layer1의 output 개수가 layer2의 input 개수와 같아야 한다.\n",
    "W2 = tf.Variable(tf.random_normal([256,256]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "\n",
    "\n",
    "# hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "# Session & 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epoch = 30\n",
    "batch_size = 100 # 55000개의 행을 다 읽어들이는게 아니라 100개의 행을 읽어서 반복 학습\n",
    "\n",
    "# 학습\n",
    "for step in range(training_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "        _,cost_val = sess.run([train,cost],\n",
    "                              feed_dict = {X:batch_x,\n",
    "                                           Y:batch_y})\n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict = {X:mnist.test.images, Y:mnist.test.labels})\n",
    "\n",
    "print(\"정확도 : {}\" .format(result))\n",
    "\n",
    "# Prediction\n",
    "## 종이에 숫자를 하나 써서 스캐너로 읽어들인 후 28*28형태의 펙셀 데이터로 변환.\n",
    "\n",
    "# # 학습 진행\n",
    "# for step in range(3000):\n",
    "#     _, cost_val = sess.run([train,cost], feed_dict={X:mnist.train.images, Y:mnist.train.labels})\n",
    "    \n",
    "#     if step%300 == 0:\n",
    "#         print(cost_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0306913\n",
      "0.008812769\n",
      "Accuracy: 0.9861190319061279\n"
     ]
    }
   ],
   "source": [
    "# 기본 MNIST(multinomial classification)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Loading\n",
    "train_data = pd.read_csv(\"./data/kaggle/train.csv\")\n",
    "train_x_data = train_data.drop('label', axis = 1)\n",
    "train_y_data = tf.one_hot(train_data[\"label\"], depth=10).eval(session = tf.Session())\n",
    "test_x_data = pd.read_csv(\"./data/kaggle/test.csv\")\n",
    "\n",
    "# Tensorflow Graph Initialization\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W1 = tf.get_variable(\"weight1\", shape = [784,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape = [256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape = [256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y))\n",
    "\n",
    "# train node\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# session object & initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch & batch size\n",
    "training_epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# training\n",
    "for step in range(training_epoch):\n",
    "    num_of_iteration = int(train_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = train_x_data[i*batch_size:(i+1)*batch_size],train_y_data[i*batch_size:(i+1)*batch_size]\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "\n",
    "    if step %5 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#predict check\n",
    "predict = tf.argmax(H,1)\n",
    "result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "df = pd.DataFrame({\n",
    "    'ImageId': [i for i in range(1,28001)],\n",
    "    'Label': result\n",
    "})\n",
    "df.to_csv('./data/kaggle/submission.csv', index=False)\n",
    "\n",
    "correct = tf.equal(predict, tf.math.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict = {X: train_x_data, Y: train_y_data, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27971</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27972</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27973</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27974</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27976</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27977</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27978</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27979</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27983</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27984</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27985</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27986</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27987</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27988</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27989</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27990</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27991</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27992</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27994</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0           0       0       0       0       0       0       0       0       0   \n",
       "1           0       0       0       0       0       0       0       0       0   \n",
       "2           0       0       0       0       0       0       0       0       0   \n",
       "3           0       0       0       0       0       0       0       0       0   \n",
       "4           0       0       0       0       0       0       0       0       0   \n",
       "5           0       0       0       0       0       0       0       0       0   \n",
       "6           0       0       0       0       0       0       0       0       0   \n",
       "7           0       0       0       0       0       0       0       0       0   \n",
       "8           0       0       0       0       0       0       0       0       0   \n",
       "9           0       0       0       0       0       0       0       0       0   \n",
       "10          0       0       0       0       0       0       0       0       0   \n",
       "11          0       0       0       0       0       0       0       0       0   \n",
       "12          0       0       0       0       0       0       0       0       0   \n",
       "13          0       0       0       0       0       0       0       0       0   \n",
       "14          0       0       0       0       0       0       0       0       0   \n",
       "15          0       0       0       0       0       0       0       0       0   \n",
       "16          0       0       0       0       0       0       0       0       0   \n",
       "17          0       0       0       0       0       0       0       0       0   \n",
       "18          0       0       0       0       0       0       0       0       0   \n",
       "19          0       0       0       0       0       0       0       0       0   \n",
       "20          0       0       0       0       0       0       0       0       0   \n",
       "21          0       0       0       0       0       0       0       0       0   \n",
       "22          0       0       0       0       0       0       0       0       0   \n",
       "23          0       0       0       0       0       0       0       0       0   \n",
       "24          0       0       0       0       0       0       0       0       0   \n",
       "25          0       0       0       0       0       0       0       0       0   \n",
       "26          0       0       0       0       0       0       0       0       0   \n",
       "27          0       0       0       0       0       0       0       0       0   \n",
       "28          0       0       0       0       0       0       0       0       0   \n",
       "29          0       0       0       0       0       0       0       0       0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "27970       0       0       0       0       0       0       0       0       0   \n",
       "27971       0       0       0       0       0       0       0       0       0   \n",
       "27972       0       0       0       0       0       0       0       0       0   \n",
       "27973       0       0       0       0       0       0       0       0       0   \n",
       "27974       0       0       0       0       0       0       0       0       0   \n",
       "27975       0       0       0       0       0       0       0       0       0   \n",
       "27976       0       0       0       0       0       0       0       0       0   \n",
       "27977       0       0       0       0       0       0       0       0       0   \n",
       "27978       0       0       0       0       0       0       0       0       0   \n",
       "27979       0       0       0       0       0       0       0       0       0   \n",
       "27980       0       0       0       0       0       0       0       0       0   \n",
       "27981       0       0       0       0       0       0       0       0       0   \n",
       "27982       0       0       0       0       0       0       0       0       0   \n",
       "27983       0       0       0       0       0       0       0       0       0   \n",
       "27984       0       0       0       0       0       0       0       0       0   \n",
       "27985       0       0       0       0       0       0       0       0       0   \n",
       "27986       0       0       0       0       0       0       0       0       0   \n",
       "27987       0       0       0       0       0       0       0       0       0   \n",
       "27988       0       0       0       0       0       0       0       0       0   \n",
       "27989       0       0       0       0       0       0       0       0       0   \n",
       "27990       0       0       0       0       0       0       0       0       0   \n",
       "27991       0       0       0       0       0       0       0       0       0   \n",
       "27992       0       0       0       0       0       0       0       0       0   \n",
       "27993       0       0       0       0       0       0       0       0       0   \n",
       "27994       0       0       0       0       0       0       0       0       0   \n",
       "27995       0       0       0       0       0       0       0       0       0   \n",
       "27996       0       0       0       0       0       0       0       0       0   \n",
       "27997       0       0       0       0       0       0       0       0       0   \n",
       "27998       0       0       0       0       0       0       0       0       0   \n",
       "27999       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "5           0  ...         0         0         0         0         0   \n",
       "6           0  ...         0         0         0         0         0   \n",
       "7           0  ...         0         0         0         0         0   \n",
       "8           0  ...         0         0         0         0         0   \n",
       "9           0  ...         0         0         0         0         0   \n",
       "10          0  ...         0         0         0         0         0   \n",
       "11          0  ...         0         0         0         0         0   \n",
       "12          0  ...         0         0         0         0         0   \n",
       "13          0  ...         0         0         0         0         0   \n",
       "14          0  ...         0         0         0         0         0   \n",
       "15          0  ...         0         0         0         0         0   \n",
       "16          0  ...         0         0         0         0         0   \n",
       "17          0  ...         0         0         0         0         0   \n",
       "18          0  ...         0         0         0         0         0   \n",
       "19          0  ...         0         0         0         0         0   \n",
       "20          0  ...         0         0         0         0         0   \n",
       "21          0  ...         0         0         0         0         0   \n",
       "22          0  ...         0         0         0         0         0   \n",
       "23          0  ...         0         0         0         0         0   \n",
       "24          0  ...         0         0         0         0         0   \n",
       "25          0  ...         0         0         0         0         0   \n",
       "26          0  ...         0         0         0         0         0   \n",
       "27          0  ...         0         0         0         0         0   \n",
       "28          0  ...         0         0         0         0         0   \n",
       "29          0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "27970       0  ...         0         0         0         0         0   \n",
       "27971       0  ...         0         0         0         0         0   \n",
       "27972       0  ...         0         0         0         0         0   \n",
       "27973       0  ...         0         0         0         0         0   \n",
       "27974       0  ...         0         0         0         0         0   \n",
       "27975       0  ...         0         0         0         0         0   \n",
       "27976       0  ...         0         0         0         0         0   \n",
       "27977       0  ...         0         0         0         0         0   \n",
       "27978       0  ...         0         0         0         0         0   \n",
       "27979       0  ...         0         0         0         0         0   \n",
       "27980       0  ...         0         0         0         0         0   \n",
       "27981       0  ...         0         0         0         0         0   \n",
       "27982       0  ...         0         0         0         0         0   \n",
       "27983       0  ...         0         0         0         0         0   \n",
       "27984       0  ...         0         0         0         0         0   \n",
       "27985       0  ...         0         0         0         0         0   \n",
       "27986       0  ...         0         0         0         0         0   \n",
       "27987       0  ...         0         0         0         0         0   \n",
       "27988       0  ...         0         0         0         0         0   \n",
       "27989       0  ...         0         0         0         0         0   \n",
       "27990       0  ...         0         0         0         0         0   \n",
       "27991       0  ...         0         0         0         0         0   \n",
       "27992       0  ...         0         0         0         0         0   \n",
       "27993       0  ...         0         0         0         0         0   \n",
       "27994       0  ...         0         0         0         0         0   \n",
       "27995       0  ...         0         0         0         0         0   \n",
       "27996       0  ...         0         0         0         0         0   \n",
       "27997       0  ...         0         0         0         0         0   \n",
       "27998       0  ...         0         0         0         0         0   \n",
       "27999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "5             0         0         0         0         0  \n",
       "6             0         0         0         0         0  \n",
       "7             0         0         0         0         0  \n",
       "8             0         0         0         0         0  \n",
       "9             0         0         0         0         0  \n",
       "10            0         0         0         0         0  \n",
       "11            0         0         0         0         0  \n",
       "12            0         0         0         0         0  \n",
       "13            0         0         0         0         0  \n",
       "14            0         0         0         0         0  \n",
       "15            0         0         0         0         0  \n",
       "16            0         0         0         0         0  \n",
       "17            0         0         0         0         0  \n",
       "18            0         0         0         0         0  \n",
       "19            0         0         0         0         0  \n",
       "20            0         0         0         0         0  \n",
       "21            0         0         0         0         0  \n",
       "22            0         0         0         0         0  \n",
       "23            0         0         0         0         0  \n",
       "24            0         0         0         0         0  \n",
       "25            0         0         0         0         0  \n",
       "26            0         0         0         0         0  \n",
       "27            0         0         0         0         0  \n",
       "28            0         0         0         0         0  \n",
       "29            0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "27970         0         0         0         0         0  \n",
       "27971         0         0         0         0         0  \n",
       "27972         0         0         0         0         0  \n",
       "27973         0         0         0         0         0  \n",
       "27974         0         0         0         0         0  \n",
       "27975         0         0         0         0         0  \n",
       "27976         0         0         0         0         0  \n",
       "27977         0         0         0         0         0  \n",
       "27978         0         0         0         0         0  \n",
       "27979         0         0         0         0         0  \n",
       "27980         0         0         0         0         0  \n",
       "27981         0         0         0         0         0  \n",
       "27982         0         0         0         0         0  \n",
       "27983         0         0         0         0         0  \n",
       "27984         0         0         0         0         0  \n",
       "27985         0         0         0         0         0  \n",
       "27986         0         0         0         0         0  \n",
       "27987         0         0         0         0         0  \n",
       "27988         0         0         0         0         0  \n",
       "27989         0         0         0         0         0  \n",
       "27990         0         0         0         0         0  \n",
       "27991         0         0         0         0         0  \n",
       "27992         0         0         0         0         0  \n",
       "27993         0         0         0         0         0  \n",
       "27994         0         0         0         0         0  \n",
       "27995         0         0         0         0         0  \n",
       "27996         0         0         0         0         0  \n",
       "27997         0         0         0         0         0  \n",
       "27998         0         0         0         0         0  \n",
       "27999         0         0         0         0         0  \n",
       "\n",
       "[28000 rows x 784 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81762975\n",
      "0.13007276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9791666865348816\n"
     ]
    }
   ],
   "source": [
    "# 기본 MNIST(multinomial classification)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Loading\n",
    "train_data = pd.read_csv(\"./data/kaggle/train.csv\")\n",
    "train_x_data = train_data.drop('label', axis = 1)\n",
    "train_y_data = tf.one_hot(train_data[\"label\"], depth=10).eval(session = tf.Session())\n",
    "test_x_data = pd.read_csv(\"./data/kaggle/test.csv\")\n",
    "df = pd.read_csv(\"./data/titanic/train.csv\", sep=\",\")\n",
    "\n",
    "\n",
    "# Tensorflow Graph Initialization\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W1 = tf.get_variable(\"weight1\", shape = [784,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape = [256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape = [256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y))\n",
    "\n",
    "# train node\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# session object & initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch & batch size\n",
    "training_epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "display(test_x_data)\n",
    "\n",
    "# training\n",
    "for step in range(training_epoch):\n",
    "    num_of_iteration = int(train_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = train_x_data[i*batch_size:(i+1)*batch_size],train_y_data[i*batch_size:(i+1)*batch_size]\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "\n",
    "    if step %5 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#predict check\n",
    "predict = tf.argmax(H,1)\n",
    "\n",
    "\n",
    "result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "display(result)\n",
    "print(\"===\")\n",
    "display(result.shape)\n",
    "df = pd.DataFrame({\n",
    "    'ImageId': [i for i in range(1,28001)],\n",
    "    'Label': result\n",
    "})\n",
    "df.to_csv('./data/kaggle/submission.csv', index=False)\n",
    "\n",
    "correct = tf.equal(predict, tf.math.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict = {X: train_x_data, Y: train_y_data, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  SibSp  Parch Embarked\n",
       "0           0       3    male  22.0      1      0        S\n",
       "1           1       1  female  38.0      1      0        C\n",
       "2           1       3  female  26.0      0      0        S\n",
       "3           1       1  female  35.0      1      0        S\n",
       "4           0       3    male  35.0      0      0        S\n",
       "6           0       1    male  54.0      0      0        S\n",
       "7           0       3    male   2.0      3      1        S\n",
       "8           1       3  female  27.0      0      2        S\n",
       "9           1       2  female  14.0      1      0        C\n",
       "10          1       3  female   4.0      1      1        S\n",
       "11          1       1  female  58.0      0      0        S\n",
       "12          0       3    male  20.0      0      0        S\n",
       "13          0       3    male  39.0      1      5        S\n",
       "14          0       3  female  14.0      0      0        S\n",
       "15          1       2  female  55.0      0      0        S\n",
       "16          0       3    male   2.0      4      1        Q\n",
       "18          0       3  female  31.0      1      0        S\n",
       "20          0       2    male  35.0      0      0        S\n",
       "21          1       2    male  34.0      0      0        S\n",
       "22          1       3  female  15.0      0      0        Q\n",
       "23          1       1    male  28.0      0      0        S\n",
       "24          0       3  female   8.0      3      1        S\n",
       "25          1       3  female  38.0      1      5        S\n",
       "27          0       1    male  19.0      3      2        S\n",
       "30          0       1    male  40.0      0      0        C\n",
       "33          0       2    male  66.0      0      0        S\n",
       "34          0       1    male  28.0      1      0        C\n",
       "35          0       1    male  42.0      1      0        S\n",
       "37          0       3    male  21.0      0      0        S\n",
       "38          0       3  female  18.0      2      0        S\n",
       "..        ...     ...     ...   ...    ...    ...      ...\n",
       "856         1       1  female  45.0      1      1        S\n",
       "857         1       1    male  51.0      0      0        S\n",
       "858         1       3  female  24.0      0      3        C\n",
       "860         0       3    male  41.0      2      0        S\n",
       "861         0       2    male  21.0      1      0        S\n",
       "862         1       1  female  48.0      0      0        S\n",
       "864         0       2    male  24.0      0      0        S\n",
       "865         1       2  female  42.0      0      0        S\n",
       "866         1       2  female  27.0      1      0        C\n",
       "867         0       1    male  31.0      0      0        S\n",
       "869         1       3    male   4.0      1      1        S\n",
       "870         0       3    male  26.0      0      0        S\n",
       "871         1       1  female  47.0      1      1        S\n",
       "872         0       1    male  33.0      0      0        S\n",
       "873         0       3    male  47.0      0      0        S\n",
       "874         1       2  female  28.0      1      0        C\n",
       "875         1       3  female  15.0      0      0        C\n",
       "876         0       3    male  20.0      0      0        S\n",
       "877         0       3    male  19.0      0      0        S\n",
       "879         1       1  female  56.0      0      1        C\n",
       "880         1       2  female  25.0      0      1        S\n",
       "881         0       3    male  33.0      0      0        S\n",
       "882         0       3  female  22.0      0      0        S\n",
       "883         0       2    male  28.0      0      0        S\n",
       "884         0       3    male  25.0      0      0        S\n",
       "885         0       3  female  39.0      0      5        Q\n",
       "886         0       2    male  27.0      0      0        S\n",
       "887         1       1  female  19.0      0      0        S\n",
       "889         1       1    male  26.0      0      0        C\n",
       "890         0       3    male  32.0      0      0        Q\n",
       "\n",
       "[712 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(712, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100, 784) for Tensor 'Placeholder:0', which has shape '(?, 7)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-6d1c873f0b04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_of_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (100, 784) for Tensor 'Placeholder:0', which has shape '(?, 7)'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"./data/titanic/train.csv\", sep=\",\" )\n",
    "\n",
    "#데이터정제\n",
    "df.drop([\"Name\",\"PassengerId\",\"Ticket\",\"Fare\",\"Cabin\"], axis=1, inplace=True)\n",
    "df.dropna(how=\"any\", inplace=True)\n",
    "# NaN이 있는 모든 row 삭제\n",
    "display(df)\n",
    "\n",
    "df.loc[df[\"Sex\"]==\"male\",\"Sex\"] = 1 #남자는 1, 여자는 2\n",
    "df.loc[df[\"Sex\"]==\"female\",\"Sex\"] = 2\n",
    "df.loc[:,\"Age\"] = (df[\"Age\"]//10) #연령을 처리하기 쉽도록 0~9세는 0, 10~19세는 1, 20~29는 2… 식으로 표현\n",
    "df.loc[df[\"Embarked\"]==\"S\",\"Embarked\"] = 1 # Embarked == 1: Southampton\n",
    "df.loc[df[\"Embarked\"]==\"C\",\"Embarked\"] = 2 # Embarked == 2: Cherbourg\n",
    "df.loc[df[\"Embarked\"]==\"Q\",\"Embarked\"] = 3 # Embarked == 3: Queenstown\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x데이터 추출\n",
    "df_x=df.drop(\"Survived\", axis=1, inplace=False)\n",
    "# # y데이터 추출\n",
    "df_y = df[\"Survived\"]\n",
    "\n",
    "display(df.shape)\n",
    "\n",
    "#training data set (Matrix형태 ) \n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_value = df_y.values\n",
    "y_data = MinMaxScaler().fit_transform(y_value.reshape(-1,1))\n",
    "\n",
    "# Tensorflow Graph Initialization\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "# Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "# keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "X = tf.placeholder(shape = [None, 7], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 1], dtype = tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "#W=tf.Variable(tf.random_normal([6,1]), name=\"weight\")\n",
    "#b=tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", shape = [7,1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable(\"bias1\", shape = [1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape = [1024,1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable(\"bias2\", shape = [1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape = [1024,1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.get_variable(\"bias3\", shape = [10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y))\n",
    "\n",
    "# train node\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# session object & initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch & batch size\n",
    "training_epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "# training\n",
    "for step in range(training_epoch):\n",
    "    num_of_iteration = int(train_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = train_x_data[i*batch_size:(i+1)*batch_size],train_y_data[i*batch_size:(i+1)*batch_size]\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "\n",
    "    if step %5 == 0:\n",
    "        print(cost_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23567428\n",
      "0.20542742\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Tensor 'Mean_1:0' shape=() dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"Mean_1:0\", shape=(), dtype=float32) is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    299\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 300\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    301\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3477\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3478\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3556\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3557\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3558\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor Tensor(\"Mean_1:0\", shape=(), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-41e4f27cc5e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;31m# accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_x_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1137\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \"\"\"\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    305\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[1;32m--> 307\u001b[1;33m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[0;32m    308\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mValueError\u001b[0m: Fetch argument <tf.Tensor 'Mean_1:0' shape=() dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"Mean_1:0\", shape=(), dtype=float32) is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "df = pd.read_csv(\"./data/titanic/train.csv\", sep=\",\" )\n",
    "\n",
    "#데이터정제\n",
    "df.drop([\"Name\",\"PassengerId\",\"Ticket\",\"Cabin\"], axis=1, inplace=True)\n",
    "# NaN이 있는 모든 row 삭제\n",
    "\n",
    "df.fillna(method=\"ffill\",inplace = True)\n",
    "\n",
    "df.loc[df[\"Sex\"]==\"male\",\"Sex\"] = 1 #남자는 1, 여자는 2\n",
    "df.loc[df[\"Sex\"]==\"female\",\"Sex\"] = 2\n",
    "df.loc[:,\"Age\"] = (df[\"Age\"]//10) #연령을 처리하기 쉽도록 0~9세는 0, 10~19세는 1, 20~29는 2… 식으로 표현\n",
    "df.loc[df[\"Embarked\"]==\"S\",\"Embarked\"] = 1 # Embarked == 1: Southampton\n",
    "df.loc[df[\"Embarked\"]==\"C\",\"Embarked\"] = 2 # Embarked == 2: Cherbourg\n",
    "df.loc[df[\"Embarked\"]==\"Q\",\"Embarked\"] = 3 # Embarked == 3: Queenstown\n",
    "df[\"Fare\"] = pd.qcut(df[\"Fare\"], 5, labels=[1,2,3,4,5])\n",
    "\n",
    "# x데이터 추출\n",
    "train_x = df.drop(\"Survived\", axis=1, inplace=False)\n",
    "\n",
    "# # y데이터 추출\n",
    "train_y = df[\"Survived\"]\n",
    "\n",
    "#training data set (Matrix형태 ) \n",
    "train_x_data = MinMaxScaler().fit_transform(train_x.values)\n",
    "train_y_value = train_y.values\n",
    "train_y_data = MinMaxScaler().fit_transform(train_y_value.reshape(-1,1))\n",
    "\n",
    "df = pd.read_csv(\"./data/titanic/test.csv\", sep=\",\" )\n",
    "df2 = df[\"PassengerId\"]\n",
    "#데이터정제\n",
    "df.drop([\"Name\",\"PassengerId\",\"Ticket\",\"Cabin\"], axis=1, inplace=True)\n",
    "\n",
    "# NaN이 있는 모든 row 삭제\n",
    "df.fillna(method=\"ffill\",inplace = True)\n",
    "df.loc[df[\"Sex\"]==\"male\",\"Sex\"] = 1 #남자는 1, 여자는 2\n",
    "df.loc[df[\"Sex\"]==\"female\",\"Sex\"] = 2\n",
    "df.loc[:,\"Age\"] = (df[\"Age\"]//10) #연령을 처리하기 쉽도록 0~9세는 0, 10~19세는 1, 20~29는 2… 식으로 표현\n",
    "df.loc[df[\"Embarked\"]==\"S\",\"Embarked\"] = 1 # Embarked == 1: Southampton\n",
    "df.loc[df[\"Embarked\"]==\"C\",\"Embarked\"] = 2 # Embarked == 2: Cherbourg\n",
    "df.loc[df[\"Embarked\"]==\"Q\",\"Embarked\"] = 3 # Embarked == 3: Queenstown\n",
    "df[\"Fare\"] = pd.qcut(df[\"Fare\"], 5, labels=[1,2,3,4,5])\n",
    "\n",
    "# x데이터 추출\n",
    "#test_x = df.drop(\"Survived\", axis=1, inplace = False)  # y데이터 추출\n",
    "test_x = df\n",
    "\n",
    "#training data set (Matrix형태 ) \n",
    "test_x_data = MinMaxScaler().fit_transform(test_x.values)\n",
    "\n",
    "# Tensorflow Graph Initialization\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(shape = [None, 7], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 1], dtype = tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "#W=tf.Variable(tf.random_normal([6,1]), name=\"weight\")\n",
    "#b=tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", shape = [7,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable(\"bias1\", shape = [256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape = [256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable(\"bias2\", shape = [256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape = [256,1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.get_variable(\"bias3\", shape = [1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "#cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits_v2(logits = logits, labels = Y))\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "# train node\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# session object & initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch & batch size\n",
    "training_epoch = 10\n",
    "\n",
    "\n",
    "# training\n",
    "for step in range(training_epoch):\n",
    "    num_of_iteration = int(train_x_data.shape[0])\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iteration):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: train_x_data, Y: train_y_data, keep_prob: 1.0})\n",
    "\n",
    "    if step %5 == 0:\n",
    "        print(cost_val)\n",
    "\n",
    "\n",
    "#predict check\n",
    "\n",
    "result = sess.run(tf.cast(sess.run(H, feed_dict = {X:test_x_data,keep_prob:1.0}) > 0.5,dtype = tf.int32))\n",
    "# print(\"===\")\n",
    "# display(result.shape)\n",
    "result = result[:,0]\n",
    "(result.shape)\n",
    "# display(result.shape)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({ \n",
    "     'PassengerId': df2,\n",
    "     'Survived': result\n",
    "})\n",
    "df.to_csv('./data/titanic/submission7.csv', index=False)\n",
    "# correct = tf.equal(predict, Y)\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict = {X: train_x_data, Y: train_y_data,keep_prob : 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7932960987091064\n",
      "<class 'numpy.ndarray'>\n",
      "(418, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "5            897         0\n",
       "6            898         0\n",
       "7            899         0\n",
       "8            900         1\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         0\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         0\n",
       "19           911         0\n",
       "20           912         0\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         0\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         0\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         0\n",
       "391         1283         1\n",
       "392         1284         0\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         0\n",
       "404         1296         0\n",
       "405         1297         0\n",
       "406         1298         0\n",
       "407         1299         0\n",
       "408         1300         0\n",
       "409         1301         1\n",
       "410         1302         0\n",
       "411         1303         1\n",
       "412         1304         0\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './data/titanic/submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-65c25ef4ac0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/titanic/submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    156\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './data/titanic/submission.csv'"
     ]
    }
   ],
   "source": [
    "## Titanic 세환햄\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, MinMaxScaler, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_data = pd.read_csv(\"./data/titanic/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/titanic/test.csv\")\n",
    "\n",
    "def fill_nan(data, columns):\n",
    "    for column in columns:\n",
    "        data[column] = Imputer().fit_transform(data[column].values.reshape(-1,1))\n",
    "    return data\n",
    "\n",
    "train_data = fill_nan(train_data, [\"Age\", \"SibSp\", \"Parch\"])\n",
    "test_data = fill_nan(test_data, [\"Age\", \"SibSp\", \"Parch\"])\n",
    "\n",
    "test_pid = test_data[\"PassengerId\"]\n",
    "train_data.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"], axis = 1, inplace = True)\n",
    "test_data.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"],axis = 1, inplace = True)\n",
    "\n",
    "def dummy_data(data, column):\n",
    "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n",
    "    data.drop(column, axis=1)\n",
    "    return data\n",
    "\n",
    "train_data = dummy_data(train_data, \"Pclass\")\n",
    "test_data = dummy_data(test_data, \"Pclass\")\n",
    "\n",
    "def sex_to_int(data):\n",
    "    le = LabelEncoder()\n",
    "    le.fit([\"male\", \"female\"])\n",
    "    data[\"Sex\"] = le.transform(data[\"Sex\"])\n",
    "    return data\n",
    "\n",
    "train_data = sex_to_int(train_data)\n",
    "test_data = sex_to_int(test_data)\n",
    "\n",
    "train_data[\"Age\"] = MinMaxScaler().fit_transform(train_data[\"Age\"].values.reshape(-1,1))\n",
    "test_data[\"Age\"] = MinMaxScaler().fit_transform(test_data[\"Age\"].values.reshape(-1,1))\n",
    "\n",
    "def split(data):\n",
    "    data_y = LabelBinarizer().fit_transform(data[\"Survived\"])\n",
    "    data_x = data.drop([\"Survived\"], axis=1)\n",
    "    \n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data_x, data_y, test_size=0.2)\n",
    "    \n",
    "    return train_x.values, train_y, valid_x, valid_y\n",
    "\n",
    "train_x, train_y, valid_x, valid_y = split(train_data)\n",
    "\n",
    "X = tf.placeholder(shape=[None, train_x.shape[1]], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", shape = [train_x.shape[1],256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape = [256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape = [256,1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([1]), name=\"bias3\")\n",
    "\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epoch = 100\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X: train_x, Y: train_y, keep_prob: 1.0})\n",
    "#     print(\"epoch: {0}, cost: {1}\".format(epoch, cost_val))\n",
    "    \n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "result = sess.run(predict, feed_dict={X: test_data, keep_prob: 1.0})\n",
    "correct = tf.equal(predict, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict={X: valid_x, Y: valid_y, keep_prob: 1.0})))\n",
    "print(type(result))\n",
    "print(result.shape)\n",
    "df = pd.DataFrame(test_pid, columns=[\"PassengerId\"])\n",
    "df_survived = pd.DataFrame(result, columns=[\"Survived\"], dtype=np.int64)\n",
    "df = df.join(df_survived)\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.to_csv(\"./data/titanic/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
