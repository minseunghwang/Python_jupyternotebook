{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a37edd82bcf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 필요한 module import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "# 필요한 module import\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) # 우리가 사용할 데이터파일의 압축파일이 쏙 들어가는 곳 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Model 정의(Tensorflow graph 생성)\n",
    "tf.reset_default_graph() # tensorflow graph 초기화\n",
    "## 2.1 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "drop_rate = tf.placeholder(dtype=tf.float32)\n",
    "## 2.2 Convolution\n",
    "## CNN은 이미지 학습에 최적화된 depp learning 방법\n",
    "## 입력받은 이미지의 형태가 4차원 매트릭스\n",
    "## (이미지의 개수, 이미지의 width, 이미지 height, color수)\n",
    "X_img=tf.reshape(X, [-1,28,28,1])\n",
    "## 2.3 Convolution\n",
    "## filter 정의 => filter의 shape()\n",
    "# filter1 = tf.Variable(tf.random_normal([3,3,1,32]))\n",
    "# ## filter를 이용해서 Convolution image를 생성\n",
    "# # 원본이미지화 필터이미지를 행렬곱 연산을해서 컨벌루젼 이미지를 만든다(정확하게는 activation이미지 생성)\n",
    "# L1 = tf.nn.conv2d(X_img,filter1,strides=[1,1,1,1], padding=\"SAME\")\n",
    "# ## 만들어진 Convolution에 Relu를 적용\n",
    "# L1 = tf.nn.relu(L1)\n",
    "# ## pooloing 작업(reesize, sampling 작업) => optional\n",
    "# L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\") # kernel 사이즈 가로2 세로2 의미\n",
    "\n",
    "# 첫번째 컨벌루젼 레이어 ~하고, 렐루하고, 풀링하고 ?\n",
    "L1 = tf.layers.conv2d(inputs=X_img, filters=3, kernel_size=[3,3], padding=\"SAME\",strides=1, activation=tf.nn.relu)\n",
    "L1 = tf.layers.max_pooling2d(inputs = L1, pool_size=[2,2], padding=\"SAME\", strides=2) # ksize를 poo_size로 바꿔ㅜ줄거\n",
    "# 레이어가 많을수록 이미지의 갯수는 많아지지만 크기가 작아지므로 데이터 손실이 일어남 => 적정한 갯수 찾아야함\n",
    "\n",
    "## Convolution Layer2\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=6, kernel_size=[3,3], padding=\"SAME\", strides=1, activation=tf.nn.relu)\n",
    "\n",
    "L2 = tf.layers.max_pooling2d(inputs = L2, pool_size=[2,2], padding=\"SAME\", strides=2) # maxpooling\n",
    "\n",
    "print(L2.shape)\n",
    "# (?,7,7,64) # 7x7 짜리가 64개 존재해\n",
    "## 2.3 Nural Network\n",
    "## Convolution의 결과(4차원)를\n",
    "## Neural Network의 입력(2차원)으로 사용하기 위해 shape을 변경\n",
    "L2 = tf.reshape(L2,[-1,7*7*6])\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", shape=[7*7*6, 256], initializer=tf.contrib.layers.xavier_initializer()) # 컬럼의갯수, 아웃풋갯수(임의)\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W1) + b1)\n",
    "layer1 = tf.layers.dropout(_layer1, rate=drop_rate)\n",
    "\n",
    "# 2번째 레이어\n",
    "W2 = tf.get_variable(\"weight2\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer()) # 컬럼의갯수, 아웃풋갯수(임의)\n",
    "b2 = tf.Variable(tf.random_normal([10]), name=\"bias2\")\n",
    "\n",
    "## Hypothesis\n",
    "logits = tf.matmul(layer1, W2) + b2\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "## cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "## train\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train=optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 진행 (batch처리)\n",
    "\n",
    "# Accuracy 흑정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 결국 우리 MNIST예제는 입력한 이미지 1개에 대해 예측한 결과가 H의 값으로 도출\n",
    "#### [0.5, 0.8, 0.99, 0.12, 0.34, ...] 총 10개 # 가장큰 0.99가 몇번째에 있는지 찾자\n",
    "\n",
    "#### 앙상블은 이런 model이 여러개 있어요!\n",
    "#### H1 => [0.5, 0.8, 0.99, 0.12, 0.34 ...]\n",
    "#### H2 => [0.2, 0.3, 0.94, 0.5, 0.1 ...]\n",
    "#### H3 => [0.7, 0.1, 0.3, 0.2, 0.12 ...]\n",
    "#### H4 => [0.26, 0.23, 0.194, 0.56, 0.31 ...]\n",
    "\n",
    "#### SUM => [1.66, 1.43, 2.4, 1.3, 1.2 ...]\n",
    "#### 최종 prediction은 SUM한 결과값을 가지고 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 기본 MNIST(multinomial classification) \n",
    "## CNN @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Loading\n",
    "train_data = pd.read_csv(\"./data/kaggle/train.csv\")\n",
    "train_x_data = train_data.drop('label', axis = 1)\n",
    "train_y_data = tf.one_hot(train_data[\"label\"], depth=10).eval(session = tf.Session())\n",
    "test_x_data = pd.read_csv(\"./data/kaggle/test.csv\")\n",
    "\n",
    "# Tensorflow Graph Initialization\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "drop_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "#Convolution\n",
    "x_img = tf.reshape(X, [-1,28,28,1])\n",
    "\n",
    "# W1 = tf.Variable(tf.random_normal(shape=[2,2,1,6]), name=\"filter1\")\n",
    "# L1 = tf.nn.conv2d(x_img, W1, strides=[1,2,2,1], padding=\"SAME\")\n",
    "# L1 = tf.nn.relu(L1)\n",
    "# L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "L1 = tf.layers.conv2d(inputs=x_img, filters=9, kernel_size=[2,2], padding=\"SAME\",strides=1, activation=tf.nn.relu)\n",
    "L1 = tf.layers.max_pooling2d(inputs = L1, pool_size=[2,2], padding=\"SAME\", strides=2) # ksize를 poo_size로 바꿔ㅜ줄거\n",
    "\n",
    "## 2.2.1 Convolution Layer2\n",
    "# W2 = tf.Variable(tf.random_normal(shape=[3,3,6,12]), name=\"filter2\")\n",
    "# L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
    "# L2 = tf.nn.relu(L2)\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=9, kernel_size=[2,2], padding=\"SAME\",strides=1, activation=tf.nn.relu)\n",
    "L2 = tf.layers.max_pooling2d(inputs = L2, pool_size=[2,2], padding=\"SAME\", strides=2)\n",
    "\n",
    "L2 = tf.reshape(L2, [-1,7*7*9])\n",
    "W3 = tf.get_variable(\"weight\", shape=[7*7*9,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal(shape=[10]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(L2,W3) + b\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y))\n",
    "\n",
    "# train node\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# session object & initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch & batch size\n",
    "training_epoch = 5 # 니가학습 몇번시킬래\n",
    "batch_size = 100 # 데이터를 몇등분해서 학습시킬래 # 환경이좋으면 작게 넣어서 한번에 여러번 학습\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# training\n",
    "for step in range(training_epoch):\n",
    "    num_of_iteration = int(train_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = train_x_data[i*batch_size:(i+1)*batch_size],train_y_data[i*batch_size:(i+1)*batch_size]\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_x, Y: batch_y, drop_rate: 0.3})\n",
    "\n",
    "    if step %2 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "saver.save(sess, './trainModel/my_test_model',global_step=1000)        \n",
    "# #predict check\n",
    "predict = tf.argmax(H,1)\n",
    "# result = sess.run(predict, feed_dict={X:test_x_data, drop_rate: 0.3})\n",
    "# df = pd.DataFrame({\n",
    "#     'ImageId': [i for i in range(1,28001)],\n",
    "#     'Label': result\n",
    "# })\n",
    "# df.to_csv('./data/kaggle/submission2.csv', index=False)\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'ImageId': [i for i in range(1,28001)],\n",
    "#     'Label': sess.run(H,feed_dict={X:test_x_data, drop_rate: 0.3})\n",
    "# })\n",
    "# df.to_csv('./data/kaggle/ms.csv', index=False)\n",
    "\n",
    "# accuracy check\n",
    "correct = tf.equal(predict, tf.math.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict = {X: train_x_data, Y: train_y_data, drop_rate:0.3})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch & batch size\n",
    "training_epoch = 5 # 니가학습 몇번시킬래\n",
    "batch_size = 100 # 데이터를 몇등분해서 학습시킬래 # 환경이좋으면 작게 넣어서 한번에 여러번 학습\n",
    "\n",
    "# training\n",
    "for step in range(training_epoch):\n",
    "    num_of_iteration = int(train_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = train_x_data[i*batch_size:(i+1)*batch_size],train_y_data[i*batch_size:(i+1)*batch_size]\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_x, Y: batch_y, drop_rate: 0.3})\n",
    "\n",
    "    if step %2 == 0:\n",
    "        print(cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-35c7f060c542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Ensemble MNIST ooo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "## Ensemble MNIST ooo\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "## Graph init\n",
    "tf.reset_default_graph()\n",
    "## Class Define\n",
    "## cnn model \n",
    "class CnnModel:\n",
    "        # constructor\n",
    "        def __init__(self,sess,name,m,test):\n",
    "            self.sess = sess\n",
    "            self.name = name\n",
    "            self.mnist = m\n",
    "            self.test_x_data = test\n",
    "            self.build_net()\n",
    "         \n",
    "        # tensorflow model graph(node) method    \n",
    "        def build_net(self):    \n",
    "            with tf.variable_scope(self.name):\n",
    "\n",
    "                # tensorflow graph init\n",
    "                self.X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "                self.Y = tf.placeholder(shape = [None,10], dtype = tf.float32)\n",
    "                self.drop_rate = tf.placeholder(dtype = tf.float32)\n",
    "                X_img = tf.reshape(self.X,[-1,28,28,1])\n",
    "\n",
    "                L1 = tf.layers.conv2d(inputs=X_img,\n",
    "                                      filters = 1,\n",
    "                                      kernel_size=[3,3],\n",
    "                                      padding= \"SAME\",\n",
    "                                      strides=1,\n",
    "                                      activation=tf.nn.relu)\n",
    "\n",
    "                L1 = tf.layers.max_pooling2d(inputs= L1,\n",
    "                                             pool_size=[2,2],\n",
    "                                             strides = 2,\n",
    "                                             padding = \"SAME\")\n",
    "\n",
    "                L2 = tf.layers.conv2d(inputs=L1,\n",
    "                                      filters = 1,\n",
    "                                      kernel_size=[3,3],\n",
    "                                      padding= \"SAME\",\n",
    "                                      strides=1,\n",
    "                                      activation=tf.nn.relu)\n",
    "\n",
    "                L2 = tf.layers.max_pooling2d(inputs= L2,\n",
    "                                             pool_size=[2,2],\n",
    "                                             strides = 2,\n",
    "                                             padding = \"SAME\")\n",
    "\n",
    "                L2 = tf.reshape(L2, [-1,7*7*1])\n",
    "\n",
    "                W1 = tf.get_variable(\"weight1\",shape = [7*7*1,32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b1 = tf.Variable(tf.random_normal([32]),name = \"bias1\")\n",
    "                _layer1 = tf.nn.relu(tf.matmul(L2,W1)+ b1)\n",
    "                layer1 = tf.layers.dropout (_layer1, rate = self.drop_rate)\n",
    "\n",
    "                W2 = tf.get_variable(\"weight2\",shape = [32,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b2 = tf.Variable(tf.random_normal([10]),name = \"bias2\")\n",
    "\n",
    "                self.logits = tf.matmul(layer1,W2)+ b2\n",
    "                self.H = tf.nn.relu(self.logits)\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.Y))\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "            self.train = self.optimizer.minimize(self.cost)\n",
    "            predict = tf.argmax(self.H,1)\n",
    "            correct = tf.equal(predict,tf.argmax(self.Y,1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "            \n",
    "        # model train\n",
    "        def train_net(self, train_x_data, train_y_data):\n",
    "            return sess.run([self.train, self.cost],feed_dict = {self.X : batch_x, self.Y: batch_y, self.drop_rate:0.7})\n",
    "           \n",
    "        \n",
    "        # model Accuracy\n",
    "        def get_accuracy(self,train_x_data, train_y_data):\n",
    "            self.result = self.sess.run(self.accuracy , feed_dict = {self.X:train_x_data,\n",
    "                                                       self.Y: train_y_data,\n",
    "                                                       self.drop_rate:0.7})\n",
    "            return self.result \n",
    "    \n",
    "        # model의 prediction\n",
    "        def get_prediction(self,x_data):\n",
    "            return self.sess.run(self.H,feed_dict={self.X:x_data,self.drop_rate:1.0})\n",
    "            \n",
    "## 1.Data loading\n",
    "mnist= input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "train= mnist.train\n",
    "test_x_data = pd.read_csv(\"./data/kaggle/test.csv\")\n",
    "# test_x_data = pd.read_csv(\"./data/digitrecognizer/test.csv\")\n",
    "\n",
    "## 2. Model number\n",
    "sess = tf.Session()\n",
    "\n",
    "training_epochs = 1\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_of_model = 2\n",
    "cnn_models = [CnnModel(sess,\"Model\" + str(x),train,test_x_data) for x in range(num_of_model)]\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('./model')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else : \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# for m in cnn_models:\n",
    "#     print(m.name)\n",
    "#     for step in range(training_epochs):\n",
    "#         num_of_iter = int(train.num_examples / batch_size)\n",
    "#         cost_val = 0\n",
    "#         for i in range(num_of_iter):\n",
    "#             batch_x, batch_y = train.next_batch(batch_size)\n",
    "#             _,cost_val = m.train_net(train.images,train.labels)\n",
    "#     print('Epoch: ', '%04d' %(step + 1), 'Cost = ', cost_val)\n",
    "#     print(m.get_accuracy(mnist.test.images,mnist.test.labels))\n",
    "# print('Training Finished')\n",
    "\n",
    "# saver.save(sess, './model/practice_model.ckpt', global_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "new_saver = tf.train.import_meta_graph('./check/practice_model.meta')\n",
    "new_saver.restore(sess, './check/practice_model')\n",
    "# tf.all_variables()\n",
    "X = sess.graph.get_tensor_by_name(\"Model0/Placeholder:0\")\n",
    "logits = sess.graph.get_tensor_by_name(\"Model0/add_1:0\")\n",
    "train = sess.graph.get_tensor_by_name(\"Model0/Placeholder_2:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (__init__.py, line 267)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\matplotlib\\__init__.py\"\u001b[1;36m, line \u001b[1;32m267\u001b[0m\n\u001b[1;33m    nonlocal called, ret\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython.display import Image\n",
    "from skimage import data, io, filters, color\n",
    "\n",
    "\n",
    "# img = Image.open('./data/number/8.png')\n",
    "# display(img)\n",
    "\n",
    "# img_test =  img.resize((28,28))\n",
    "# img = np.array(img_test)\n",
    "# img_test = color.rgb2gray(img)\n",
    "\n",
    "# io.imshow(img_test)\n",
    "# plt.show()\n",
    "\n",
    "# img_test = img_test.astype(np.float32)\n",
    "# test_img = img_test.reshape(-1, 784)\n",
    "# test_img = 1-test_img\n",
    "# print(test_img.shape)\n",
    "# display(test_img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(cnn_models.size)\n",
    "\n",
    "# mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) # 우리가 사용할 데이터파일의 압축파일이 쏙 들어가는 곳 ?\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "# new_saver = tf.train.import_meta_graph('./practice_model.meta')\n",
    "# new_saver.restore(sess, './practice_model')\n",
    "\n",
    "# X = sess.graph.get_tensor_by_name(\"Model0/Placeholder:0\")\n",
    "# logits = sess.graph.get_tensor_by_name(\"Model0/add_1:0\")\n",
    "# train = sess.graph.get_tensor_by_name(\"Model0/Placeholder_2:0\")\n",
    "# accuracy = sess.graph.get_tensor_by_name(\"Mean_1:0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_prediction(self,x_data):\n",
    "#     return self.sess.run(self.H,feed_dict={self.X:x_data,self.drop_rate:0.7})\n",
    "\n",
    "\n",
    "# img2 = io.imread(\"./data/number2/0.png\", as_gray=True) # bmp 넘파이로 드러와서 리싸이즈가 안대유\n",
    "img2 = Image.open('./data/number/0.png')\n",
    "img_test =  img2.resize((28,28))\n",
    "img2_test = color.rgb2gray(img_test)\n",
    "img3 = np.array(img2_test)\n",
    "\n",
    "print(img3)\n",
    "\n",
    "io.imshow(img2)\n",
    "plt.show()\n",
    "# print(img2)\n",
    "img2= 1-img2\n",
    "# print(img2)\n",
    "x_data = img2.reshape(-1,784)\n",
    "\n",
    "for i in cnn_models:\n",
    "    result=i.get_prediction(x_data)\n",
    "    print(sess.run(tf.argmax(result,1)))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Image' has no attribute 'open'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a9bfebbd2980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/number/0.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mpix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Image' has no attribute 'open'"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# import sys\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython.display import Image\n",
    "from skimage import data, io, filters, color\n",
    "\n",
    "\n",
    "im = Image.open('./data/number/0.png')\n",
    "\n",
    "pix = np.array(im)\n",
    "\n",
    "print(pix-250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
